Trec_eval is a tool used to evaluate the performance of information retrieval systems. It takes as input a results file
from an information retrieval run and a set of judged results (known as qrels) to calculate various evaluation metrics.
Here's how you can use trec_eval:

1. Installation: If trec_eval is not already installed, you can compile it by running the following command in the source
directory:

   make

   This should create the trec_eval binary.

2. Testing: You can run a quick test to ensure that trec_eval is working correctly. In the source directory, you can use
 the following command:


   make quicktest


   This will perform some sample simple evaluations and compare the results.

3. Basic Usage: The most common use of trec_eval is to evaluate a retrieval run with the official qrels file. The command
 typically looks like this:


   trec_eval -q -c -M1000 official_qrels submitted_results


   - -q: This flag indicates whether to output official results for individual queries as well as the averages over all
   queries.
   - -c: If submitted_results doesn't have results for all queries, this flag ensures correct evaluation.
   - -M1000: Limits the maximum number of documents per query to 1000.

   Replace official_qrels with the path to your qrels file and submitted_results with the path to your results file.

4. Specific Measure: If you want to calculate a specific measure, you can use the -m flag. For example:


   trec_eval -m ndcg official_qrels submitted_results


   This will calculate the Normalized Discounted Cumulative Gain (NDCG) measure.

   You can specify a measure and its parameters by using the -m flag followed by the measure name, like -m measure[.params].

5. Custom Formats: If you have custom formats for your qrels or results files, you can create procedures to read them as
described in the "Adding a new file format" section in the documentation.

6. Adding a New Measure: If you want to add a new evaluation measure, you can follow the guidelines provided in the
"Adding a new measure" section in the documentation. This involves creating procedures to initialize, calculate,
accumulate, and print the new measure.

Remember to replace official_qrels and submitted_results with the actual paths to your qrels and results files.
Additionally, make sure you understand the specific measures you want to calculate and their parameters if needed.

Please note that trec_eval is a command-line tool, so you should run it in a terminal or command prompt.























































public void run() {
    try {
        // Elabora le query e scrivi i risultati su un file specifico per il thread
        String outputFile = "data/collection/results_thread_" + threadId + ".txt";
        BufferedWriter writer = new BufferedWriter(new FileWriter(outputFile));

        for (String query : thread_queries) {
            String[] split = query.split("\t");
            String queryId = split[0];
            String queryText = split[1];

            this.thread_queryIDs.add(queryId);
            Query queryObj = new Query(queryText);
            ArrayList<String> queryTerms = queryObj.getQueryTerms();
            writer.write(queryTerms.toString());
            writer.newLine();

            this.thread_searcher.DAAT_block(queryTerms, this.thread_lexicon, this.thread_documents, this.thread_n_results, this.thread_mode);
            ArrayList<QueryResult> queryResults = this.thread_searcher.getQueryResults();

            for (int j = 0; j < queryResults.size(); j++) {
                QueryResult result = queryResults.get(j);
                String line = this.thread_queryIDs.get(i) + "\tQ0\t" + result.getDocNo() + "\t" + (j + 1) + "\t" + result.getScoring() + "\tSTANDARD\n";
                writer.write(line);
                writer.newLine();
            }
        }
        writer.close();
        Logs log = new Logs();
        log.getLog("Thread " + threadId + " ha completato l'elaborazione.");
    } catch (IOException e) {
        e.printStackTrace();
    }
}
